# Near-Memory Computing with Pseudoscopic

**Data doesn't move. Computation comes to the data.**

---

## The Unconventional Insight

Traditional GPU computing has a dirty secret: **data movement dominates**.

```
Traditional Pipeline:
  CPU RAM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ GPU VRAM
           50 GB transfer @ 12 GB/s = 4.2 seconds
                              â”‚
                              â–¼
                      [GPU Processing]
                         ~0.2 seconds
                              â”‚
                              â–¼
  CPU RAM â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPU VRAM
           Results transfer @ 12 GB/s = variable

  Total: Transfer time >> Compute time
```

For a 50 GB log file, you spend **4+ seconds** just moving data across PCIe, then maybe 200ms actually processing it.

**Near-Memory Computing inverts this:**

```
Near-Memory Pipeline:
  [VRAM] â† CPU writes via mmap (only what changes)
  [VRAM] â† GPU computes in-place (700 GB/s internal)
  [VRAM] â†’ CPU reads results (only what's needed)

  Total: Zero round-trip transfers
```

The GPU becomes a **memory-side accelerator**â€”a coprocessor that transforms data in-place while the CPU orchestrates.

---

## How It Works

### The Physical Setup

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GPU Card                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                        VRAM                              â”‚    â”‚
â”‚  â”‚   (HBM2 @ 700 GB/s internal bandwidth)                  â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚    â”‚
â”‚  â”‚   â”‚   Your Data        â”‚    â”‚   GPU Kernels      â”‚      â”‚    â”‚
â”‚  â”‚   â”‚   Lives Here       â”‚â—„â”€â”€â”€â”‚   Operate Here     â”‚      â”‚    â”‚
â”‚  â”‚   â”‚                    â”‚    â”‚   (No copy!)       â”‚      â”‚    â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚    â”‚
â”‚  â”‚              â–²                                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                 â”‚                                                 â”‚
â”‚           BAR1 Window (PCIe)                                     â”‚
â”‚                 â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
            â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
            â”‚    CPU    â”‚
            â”‚   mmap()  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Pseudoscopic exposes GPU VRAM as a block device (`/dev/psdisk0`). When you mmap that device, you get a CPU pointer to GPU memory. CUDA can access the exact same physical memory.

**Same bytes. Two access paths. No copy.**

### The API Flow

```c
// 1. Initialize: Open device, establish CUDA context
nearmem_ctx_t ctx;
nearmem_init(&ctx, "/dev/psdisk0", 0);

// 2. Allocate: Get a region accessible by both CPU and GPU
nearmem_region_t region;
nearmem_alloc(&ctx, &region, 16 * 1024 * 1024 * 1024);  // 16 GB

// 3. CPU writes data (goes directly to VRAM via BAR1)
memcpy(region.cpu_ptr, my_data, data_size);

// 4. Sync: Ensure CPU writes are visible to GPU
nearmem_sync(&ctx, NEARMEM_SYNC_CPU_TO_GPU);

// 5. GPU processes in-place (no transfer!)
nearmem_histogram(&ctx, &region, histogram);

// 6. Sync: Ensure GPU writes are visible to CPU
nearmem_sync(&ctx, NEARMEM_SYNC_GPU_TO_CPU);

// 7. CPU reads results (only touches the histogram, not 16 GB)
printf("Byte 0x41 count: %lu\n", histogram['A']);
```

---

## Performance Comparison

### Log Analysis (50 GB access logs)

| Approach | Time | PCIe Traffic |
|----------|------|--------------|
| Traditional (copy-process-copy) | 4.2s transfer + 0.2s compute = **4.4s** | 100 GB |
| Near-Memory | 0.2s compute + ~0s result read = **0.2s** | ~0 GB |
| **Speedup** | **22Ã—** | **âˆ** |

### Pattern Search (grep for "ERROR")

| Approach | Time for 50 GB |
|----------|----------------|
| CPU grep | ~45 seconds |
| GPU (traditional copy) | 4.2s + 0.3s + 0.01s = 4.5s |
| GPU (near-memory) | 0.3s |
| **Speedup vs CPU** | **150Ã—** |

### When Near-Memory Wins

âœ… **Ideal workloads:**
- Log analysis (grep, histogram, pattern matching)
- Data validation (checksums, format verification)
- In-place transformations (encoding, compression)
- Large dataset filtering (select rows matching criteria)
- Bulk sorting
- Deduplication

âŒ **Not ideal for:**
- Iterative algorithms with CPU feedback each iteration
- Workloads that genuinely need results in CPU RAM
- Small datasets (< 1 GB) where transfer overhead is negligible

---

## Quick Start

### Prerequisites

```bash
# 1. Load pseudoscopic in ramdisk mode
sudo modprobe pseudoscopic mode=ramdisk

# 2. Verify device exists
ls -la /dev/psdisk0

# 3. Check CUDA is available
nvidia-smi
```

### Build

```bash
cd contrib/nearmem
make

# If CUDA isn't in default location:
make CUDA_PATH=/opt/cuda-12.0
```

### Run Example

```bash
# Load some data into VRAM
sudo dd if=/var/log/syslog of=/dev/psdisk0 bs=1M

# Run the log analyzer
./log_analyzer /dev/psdisk0 "error"
```

---

## API Reference

### Initialization

```c
// Initialize with explicit device and CUDA device ID
nearmem_error_t nearmem_init(nearmem_ctx_t *ctx, 
                              const char *device_path,
                              int cuda_device);

// Auto-detect pseudoscopic device
nearmem_error_t nearmem_init_auto(nearmem_ctx_t *ctx);

// Clean up
void nearmem_shutdown(nearmem_ctx_t *ctx);
```

### Memory Management

```c
// Allocate shared region
nearmem_error_t nearmem_alloc(nearmem_ctx_t *ctx,
                               nearmem_region_t *region,
                               size_t size);

// Free region
void nearmem_free(nearmem_ctx_t *ctx, nearmem_region_t *region);

// Map specific offset (for pre-loaded data)
nearmem_error_t nearmem_map_offset(nearmem_ctx_t *ctx,
                                    nearmem_region_t *region,
                                    uint64_t offset,
                                    size_t size);
```

### Synchronization

```c
// Sync CPU writes â†’ GPU visibility
nearmem_sync(&ctx, NEARMEM_SYNC_CPU_TO_GPU);

// Sync GPU writes â†’ CPU visibility
nearmem_sync(&ctx, NEARMEM_SYNC_GPU_TO_CPU);

// Full bidirectional sync
nearmem_sync(&ctx, NEARMEM_SYNC_FULL);
```

### Built-in Operations

```c
// Memory operations
nearmem_memset(ctx, region, value, offset, size);
nearmem_memcpy(ctx, dst, dst_off, src, src_off, size);

// Search
nearmem_find(ctx, region, pattern, len, &result);
nearmem_count_matches(ctx, region, pattern, len, &count);

// Transform
nearmem_transform(ctx, region, lut);  // Apply 256-byte LUT

// Analysis
nearmem_histogram(ctx, region, histogram);
nearmem_reduce_sum_f32(ctx, region, count, &sum);

// Sort
nearmem_sort_u32(ctx, region, count);
```

---

## Architecture Deep Dive

### Why This Works

1. **BAR1 is a window, not a copy**
   - PCIe BAR1 maps directly to GPU VRAM
   - CPU writes go straight to GPU memory (write-combining)
   - No intermediate buffers

2. **CUDA sees the same physical memory**
   - CUDA device pointers are offsets into VRAM
   - Our mmap offsets correspond to the same locations
   - Same bytes, different address spaces

3. **Synchronization handles coherency**
   - CPU write-combine buffers need explicit flush
   - GPU caches need explicit invalidation
   - `nearmem_sync()` wraps the ugly details

### The Synchronization Dance

```
CPU writes to region
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Write-Combine    â”‚  â† Writes accumulate in WC buffer
â”‚     Buffers       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
    sfence + msync      â† NEARMEM_SYNC_CPU_TO_GPU
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      VRAM         â”‚  â† Data now in GPU memory
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
   GPU kernel runs
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GPU L2 Cache    â”‚  â† Results may be cached
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
   cudaDeviceSynchronize  â† NEARMEM_SYNC_GPU_TO_CPU
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      VRAM         â”‚  â† Results in GPU memory
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
    CPU reads via mmap (uncached access through BAR1)
```

---

## Use Cases

### 1. Log Processing Pipeline

```c
// Load logs directly to VRAM (disk â†’ GPU, bypassing CPU RAM)
system("dd if=access.log of=/dev/psdisk0 bs=1M");

// Map the logs
nearmem_region_t logs;
nearmem_map_offset(&ctx, &logs, 0, log_size);

// GPU-accelerated grep
nearmem_find(&ctx, &logs, "ERROR", 5, &first_error);

// GPU-accelerated stats
nearmem_histogram(&ctx, &logs, byte_histogram);

// Only read the results you need (bytes, not gigabytes)
printf("Found error at offset %ld\n", first_error);
```

### 2. Neural Network Inference Cache

```c
// Model weights live in VRAM
nearmem_region_t weights;
nearmem_alloc(&ctx, &weights, model_size);
memcpy(weights.cpu_ptr, model_data, model_size);
nearmem_sync(&ctx, NEARMEM_SYNC_CPU_TO_GPU);

// Inference: GPU reads weights, CPU reads outputs
for (batch : data) {
    // Activations computed in VRAM
    inference_kernel<<<...>>>(weights.gpu_ptr, batch, output);
    
    // Only copy final output (small) to CPU
    cudaMemcpy(result, output, output_size, cudaMemcpyDeviceToHost);
}
```

### 3. Database Buffer Extension

```c
// Cold pages live in VRAM
nearmem_region_t cold_buffer;
nearmem_alloc(&ctx, &cold_buffer, 16ULL * 1024 * 1024 * 1024);

// Page-in on CPU access (via mmap fault handling)
// Page-out to VRAM when RAM pressure (via madvise)

// GPU can do parallel scans on cold data
nearmem_find(&ctx, &cold_buffer, key, key_len, &offset);
```

---

## Comparison with Alternatives

| Approach | Data Copy | GPU Acceleration | Transparency |
|----------|-----------|------------------|--------------|
| Standard CUDA | 2Ã— PCIe | âœ“ | Manual |
| Unified Memory | Automatic | âœ“ | High |
| **Near-Memory** | **0** | âœ“ | Medium |
| NVMe direct | 0 | âœ— | Low |

**Near-Memory wins when:**
- Dataset is larger than CPU RAM
- Dataset is already in VRAM (via ramdisk mode)
- You want to avoid PCIe round-trips
- GPU acceleration provides significant speedup

---

## Limitations

1. **Requires pseudoscopic driver**
   - Kernel module must be loaded
   - GPU must not be in use by other drivers (nouveau, nvidia)

2. **CUDA device must match pseudoscopic GPU**
   - Multi-GPU systems need careful device selection

3. **Write-combining semantics**
   - CPU writes may reorder (use sync barriers)
   - Reads are uncached (high latency for random access)

4. **No automatic migration**
   - Unlike Unified Memory, data stays where you put it
   - Explicit sync calls required

---

## Future Work

- [ ] AVX-512 optimized CPU fallback paths
- [ ] Async streaming API
- [ ] Multi-GPU support
- [ ] Python bindings (numpy interop)
- [ ] Integration with Apache Arrow
- [ ] RDMA support for distributed near-memory

---

## License

MIT License - Use freely, attribution appreciated.

---

## Part of Neural Splines

Near-Memory Computing is a component of the [Pseudoscopic](https://pseudoscopic.ai) project, which is part of [Neural Splines](https://neuralsplines.com) research.

**Asymmetric solutions.**

*Data doesn't move. Computation comes to the data.* ğŸª
