/*
 * gpuCPU - x86 Emulation via Near-Memory GPU Computing
 *
 * "Any sufficiently advanced GPU is indistinguishable from a CPU."
 *                                        - Arthur C. Clarke (paraphrased)
 *
 * THE INSIGHT:
 * ============
 * A GPU is not just a graphics accelerator. It's a massively parallel
 * processor with very fast memory. What if we use it to emulate a CPU?
 *
 *   Traditional view:   CPU controls GPU for parallel workloads
 *   Inverted view:      GPU IS the CPU, VRAM IS the RAM
 *
 * THE ARCHITECTURE:
 * =================
 *
 *   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 *   â”‚                     GPU VRAM (16-80 GB HBM2)                      â”‚
 *   â”‚                     Accessed via pseudoscopic BAR1               â”‚
 *   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 *   â”‚  x86 State   â”‚ Guest Memory  â”‚  Decode Cache â”‚  I/O Buffers     â”‚
 *   â”‚  (64 bytes)  â”‚ (up to 64 GB) â”‚  (LUT tables) â”‚  (devices)       â”‚
 *   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 *          â†‘               â†‘               â†‘               â†‘
 *          â”‚               â”‚               â”‚               â”‚
 *   â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 *   â”‚                     GPU COMPUTE (CUDA cores)                     â”‚
 *   â”‚                                                                  â”‚
 *   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
 *   â”‚   â”‚ Fetch   â”‚â†’ â”‚ Decode  â”‚â†’ â”‚ Execute â”‚â†’ â”‚Writebackâ”‚            â”‚
 *   â”‚   â”‚ Thread  â”‚  â”‚ Thread  â”‚  â”‚ Threads â”‚  â”‚ Thread  â”‚            â”‚
 *   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
 *   â”‚                                                                  â”‚
 *   â”‚   Each x86 instruction â†’ GPU warp (32 threads) for execution    â”‚
 *   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 *          â†‘
 *          â”‚ Control (start/stop, interrupts)
 *   â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
 *   â”‚  Host CPU   â”‚  (Minimal role: I/O, interrupts, bootstrap)
 *   â”‚  via BAR1   â”‚
 *   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 *
 * WHY THIS COULD WORK:
 * ====================
 *
 * 1. MEMORY BANDWIDTH
 *    - x86 is often memory-bound (cache misses hurt)
 *    - GPU HBM2: 700+ GB/s vs DDR4: 50 GB/s
 *    - Emulated RAM accesses are 14Ã— faster!
 *
 * 2. PARALLELISM IN INTERPRETATION
 *    - Fetch/decode/execute can pipeline across warps
 *    - Memory operations parallelize naturally
 *    - REP MOVSB? That's a perfect GPU workload!
 *
 * 3. DETERMINISM
 *    - No branch prediction (we're interpreting anyway)
 *    - No speculative execution (Spectre-immune!)
 *    - Predictable, auditable execution
 *
 * 4. MEMORY CAPACITY
 *    - Tesla P100: 16 GB (decent VM)
 *    - Tesla V100: 32 GB (comfortable)
 *    - A100: 80 GB (run anything)
 *
 * THE EXECUTION MODEL:
 * ====================
 *
 *   // GPU kernel: The CPU emulation loop
 *   __global__ void x86_execute(x86_state_t *state, uint8_t *ram) {
 *       while (!state->halted) {
 *           // Fetch
 *           uint8_t *ip = ram + state->rip;
 *           
 *           // Decode (via LUT in constant memory)
 *           x86_uop_t uops[8];
 *           int n_uops = decode(ip, uops);
 *           
 *           // Execute (parallel across threads)
 *           for (int i = 0; i < n_uops; i++) {
 *               execute_uop(state, ram, &uops[i]);
 *           }
 *           
 *           // Advance
 *           state->rip += instruction_length;
 *       }
 *   }
 *
 * TILED EXECUTION:
 * ================
 * Large memory operations become tiled GPU kernels:
 *
 *   REP MOVSB (copy 1 GB):
 *   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 *   â”‚  Tile 0    â”‚  Tile 1    â”‚  ...  â”‚  Tile N   â”‚
 *   â”‚  64K copy  â”‚  64K copy  â”‚       â”‚  64K copy â”‚
 *   â”‚  warp 0    â”‚  warp 1    â”‚       â”‚  warp N   â”‚
 *   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 *   
 *   All tiles execute in parallel on GPU!
 *   Traditional CPU: sequential, cache-bound
 *   GPU-CPU: parallel, bandwidth-bound (14Ã— faster memory)
 *
 * NEAR-MEMORY INTEGRATION:
 * ========================
 * Pseudoscopic makes this possible:
 *
 *   1. Emulated RAM lives in VRAM (via /dev/psdisk0)
 *   2. Host CPU can inspect/modify state via BAR1 mmap
 *   3. GPU executes instructions on VRAM-resident state
 *   4. No copies! Data never moves.
 *
 *   // Host loads a binary into "RAM"
 *   memcpy(vram + GUEST_RAM_OFFSET, elf_binary, binary_size);
 *   nearmem_sync(ctx, NEARMEM_SYNC_CPU_TO_GPU);
 *   
 *   // GPU starts executing from entry point
 *   state->rip = entry_point;
 *   launch_x86_kernel(state, vram);
 *   
 *   // Host reads results
 *   nearmem_sync(ctx, NEARMEM_SYNC_GPU_TO_CPU);
 *   printf("Result: %d\n", *(int*)(vram + result_address));
 *
 * THE RECURSIVE POSSIBILITY:
 * ==========================
 * Here's where it gets mind-bending...
 *
 * If we can emulate x86 on GPU, and that x86 can run Linux...
 * And Linux can load CUDA drivers...
 * Then the emulated x86 could program THE SAME GPU it's running on.
 *
 *   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 *   â”‚  Emulated x86 Linux                         â”‚
 *   â”‚    â†“                                        â”‚
 *   â”‚  CUDA driver (in emulation)                 â”‚
 *   â”‚    â†“                                        â”‚
 *   â”‚  Programs GPU (which is running the emu!)   â”‚
 *   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 *
 * This is the ouroboros. The snake eating its tail.
 * But it might actually work for bootstrapping scenarios.
 *
 * PERFORMANCE EXPECTATIONS:
 * =========================
 *
 *   Workload Type          | vs Native x86 | Why
 *   -----------------------|---------------|---------------------------
 *   Memory-bound (memcpy)  | 5-10Ã— FASTER  | HBM2 bandwidth dominates
 *   Integer arithmetic     | 0.1-0.5Ã—      | Interpretation overhead
 *   Floating point         | 0.5-2Ã—        | GPU FPUs are fast
 *   SIMD (AVX)             | 1-5Ã—          | Maps well to GPU warps
 *   Branch-heavy code      | 0.01-0.1Ã—     | GPU hates divergence
 *   I/O-bound              | Similar       | Limited by devices anyway
 *
 * Sweet spot: Memory-intensive workloads with predictable control flow.
 * Worst case: Tight loops with unpredictable branches.
 *
 * USE CASES:
 * ==========
 *
 * 1. LEGACY CODE EXECUTION
 *    - Run 32-bit x86 binaries on GPU-only systems
 *    - Emulate vintage DOS/Windows for preservation
 *
 * 2. SECURITY SANDBOX
 *    - No speculative execution = no Spectre/Meltdown
 *    - Fully deterministic execution for analysis
 *    - Memory isolated in VRAM
 *
 * 3. MASSIVE PARALLELISM
 *    - Emulate 1000 x86 cores simultaneously
 *    - Each GPU SM runs one emulated CPU
 *    - Perfect for embarrassingly parallel legacy code
 *
 * 4. MEMORY-INTENSIVE WORKLOADS
 *    - Databases with huge working sets
 *    - In-memory analytics
 *    - Benefit from 80 GB HBM2 + 2 TB/s bandwidth
 *
 * 5. THE BOOTSTRAP SCENARIO
 *    - GPU-native systems that need to run x86 occasionally
 *    - "GPU-first" computing with x86 compatibility layer
 *
 * IMPLEMENTATION ROADMAP:
 * =======================
 *
 * Phase 1: Proof of Concept
 *   [ ] 8086 subset (16-bit real mode)
 *   [ ] Basic instructions (MOV, ADD, SUB, JMP, CALL)
 *   [ ] Memory operations via near-memory API
 *   [ ] Run simple .COM executables
 *
 * Phase 2: Protected Mode
 *   [ ] 386 protected mode
 *   [ ] Paging (via GPU memory management)
 *   [ ] Privilege levels
 *   [ ] Run Linux kernel boot sequence
 *
 * Phase 3: Long Mode (x86-64)
 *   [ ] 64-bit registers and addressing
 *   [ ] SSE/AVX (map to GPU vector ops)
 *   [ ] System calls (trap to host)
 *   [ ] Full Linux userspace
 *
 * Phase 4: Optimization
 *   [ ] JIT compilation (x86 â†’ GPU kernels)
 *   [ ] Basic block caching
 *   [ ] Memory access coalescing
 *   [ ] Speculative decode (not execute!)
 *
 * THE DEEPER MEANING:
 * ===================
 *
 * This isn't just about emulation. It's about questioning the
 * fundamental distinction between CPU and GPU.
 *
 * A "CPU" is just:
 *   - A state machine (registers)
 *   - Connected to memory
 *   - That executes instructions
 *
 * A GPU has all of these. The instruction set is different, but
 * we can implement any instruction set in software.
 *
 * The real question: Why do we need CPUs at all?
 *
 * Answer: We don't. We just need:
 *   1. Fast memory (HBM2 âœ“)
 *   2. Compute units (CUDA cores âœ“)
 *   3. I/O connectivity (PCIe âœ“)
 *   4. Software that ties it together (this project)
 *
 * The GPU IS a computer. We've just been using it wrong.
 *
 * RELATION TO NEURAL SPLINES:
 * ===========================
 *
 * Robert's Neural Splines compression achieves 128Ã— parameter reduction.
 * Combined with GPU-as-CPU:
 *
 *   Traditional: 70B model needs 140 GB VRAM + massive CPU
 *   Neural Splines + gpuCPU: 
 *     - Model compressed to ~1 GB
 *     - Inference runs on GPU
 *     - "CPU" emulated on same GPU
 *     - EVERYTHING runs on a single $200 GPU
 *
 * This is the democratization of compute.
 * No separate CPU needed. No expensive RAM.
 * Just a GPU, some VRAM, and elegant mathematics.
 *
 * ğŸª Cookie Monster's Final Theorem:
 *    "Me not need expensive computer. Me just need cookie."
 *    (Where cookie = GPU + pseudoscopic + neural splines)
 *
 * Copyright (C) 2025 Neural Splines LLC
 * License: MIT
 */

#ifndef _GPUCPU_H_
#define _GPUCPU_H_

#include <stdint.h>
#include <stdbool.h>
#include "nearmem.h"

#ifdef __cplusplus
extern "C" {
#endif

/*
 * ============================================================
 * x86 State (lives in VRAM)
 * ============================================================
 */

/* 64-bit register file */
typedef struct {
    /* General purpose (64-bit, but we access 8/16/32/64 views) */
    uint64_t rax, rbx, rcx, rdx;
    uint64_t rsi, rdi, rbp, rsp;
    uint64_t r8, r9, r10, r11;
    uint64_t r12, r13, r14, r15;
    
    /* Instruction pointer */
    uint64_t rip;
    
    /* Flags */
    uint64_t rflags;
    
    /* Segment registers (for real/protected mode) */
    uint16_t cs, ds, es, fs, gs, ss;
    
    /* Segment descriptors (protected mode) */
    uint64_t cs_base, ds_base, es_base, fs_base, gs_base, ss_base;
    uint32_t cs_limit, ds_limit, es_limit, fs_limit, gs_limit, ss_limit;
    
    /* Control registers */
    uint64_t cr0, cr2, cr3, cr4;
    
    /* Model-specific registers (subset) */
    uint64_t efer;      /* Extended Feature Enable Register */
    
    /* FPU state (simplified) */
    double fpr[8];      /* x87 FPU registers */
    uint16_t fpu_cw;    /* Control word */
    uint16_t fpu_sw;    /* Status word */
    
    /* SSE state */
    __attribute__((aligned(16))) uint8_t xmm[16][16];  /* XMM0-XMM15 */
    uint32_t mxcsr;     /* SSE control/status */
    
    /* Execution state */
    bool halted;
    bool interrupt_pending;
    uint8_t interrupt_vector;
    
    /* Statistics */
    uint64_t instructions_executed;
    uint64_t cycles;
} x86_state_t;

/* FLAGS bits */
#define FLAG_CF     (1 << 0)    /* Carry */
#define FLAG_PF     (1 << 2)    /* Parity */
#define FLAG_AF     (1 << 4)    /* Auxiliary carry */
#define FLAG_ZF     (1 << 6)    /* Zero */
#define FLAG_SF     (1 << 7)    /* Sign */
#define FLAG_TF     (1 << 8)    /* Trap */
#define FLAG_IF     (1 << 9)    /* Interrupt enable */
#define FLAG_DF     (1 << 10)   /* Direction */
#define FLAG_OF     (1 << 11)   /* Overflow */

/* CR0 bits */
#define CR0_PE      (1 << 0)    /* Protected mode enable */
#define CR0_PG      (1 << 31)   /* Paging enable */

/* EFER bits */
#define EFER_LME    (1 << 8)    /* Long mode enable */
#define EFER_LMA    (1 << 10)   /* Long mode active */

/*
 * ============================================================
 * Memory Map (in VRAM)
 * ============================================================
 */

typedef struct {
    /* Regions in VRAM */
    uint64_t state_offset;      /* x86_state_t */
    uint64_t ram_offset;        /* Guest RAM */
    uint64_t ram_size;          /* Guest RAM size */
    uint64_t decode_cache;      /* Decoded instruction cache */
    uint64_t io_buffers;        /* Device I/O buffers */
    
    /* Near-memory handles */
    nearmem_ctx_t *nm_ctx;
    nearmem_region_t state_region;
    nearmem_region_t ram_region;
} gpucpu_memory_t;

/*
 * ============================================================
 * Instruction Decoding
 * ============================================================
 */

/* Micro-operation types */
typedef enum {
    UOP_NOP,
    UOP_MOV_REG_REG,
    UOP_MOV_REG_IMM,
    UOP_MOV_REG_MEM,
    UOP_MOV_MEM_REG,
    UOP_MOV_MEM_IMM,
    UOP_ADD, UOP_SUB, UOP_AND, UOP_OR, UOP_XOR,
    UOP_CMP, UOP_TEST,
    UOP_INC, UOP_DEC, UOP_NEG, UOP_NOT,
    UOP_SHL, UOP_SHR, UOP_SAR, UOP_ROL, UOP_ROR,
    UOP_MUL, UOP_IMUL, UOP_DIV, UOP_IDIV,
    UOP_PUSH, UOP_POP,
    UOP_JMP, UOP_JCC, UOP_CALL, UOP_RET,
    UOP_SYSCALL, UOP_INT,
    UOP_REP_MOVS, UOP_REP_STOS, UOP_REP_CMPS,
    UOP_CPUID, UOP_RDTSC,
    /* ... many more ... */
} uop_type_t;

/* Operand types */
typedef enum {
    OP_NONE,
    OP_REG8, OP_REG16, OP_REG32, OP_REG64,
    OP_MEM8, OP_MEM16, OP_MEM32, OP_MEM64,
    OP_IMM8, OP_IMM16, OP_IMM32, OP_IMM64,
} operand_type_t;

/* Decoded micro-operation */
typedef struct {
    uop_type_t type;
    operand_type_t dst_type, src_type;
    uint8_t dst_reg, src_reg;   /* Register indices */
    int64_t imm;                /* Immediate value */
    uint64_t mem_addr;          /* Memory address (if computed) */
    uint8_t size;               /* Operation size in bytes */
} x86_uop_t;

/* Decoded instruction */
typedef struct {
    uint8_t length;             /* Instruction length in bytes */
    uint8_t n_uops;             /* Number of micro-ops */
    x86_uop_t uops[8];          /* Up to 8 micro-ops per instruction */
    uint8_t prefix_rex;         /* REX prefix */
    uint8_t prefix_seg;         /* Segment override */
    bool prefix_lock;           /* LOCK prefix */
    bool prefix_rep;            /* REP/REPNE prefix */
} x86_decoded_t;

/*
 * ============================================================
 * Emulator Context
 * ============================================================
 */

typedef struct {
    /* Memory layout */
    gpucpu_memory_t memory;
    
    /* State pointer (CPU-accessible via BAR1) */
    x86_state_t *state;
    
    /* Guest RAM pointer (CPU-accessible via BAR1) */
    uint8_t *ram;
    
    /* Execution mode */
    enum {
        MODE_REAL,          /* 16-bit real mode */
        MODE_PROTECTED,     /* 32-bit protected mode */
        MODE_LONG,          /* 64-bit long mode */
    } mode;
    
    /* I/O callbacks (handled by host CPU) */
    uint8_t (*io_read)(void *ctx, uint16_t port);
    void (*io_write)(void *ctx, uint16_t port, uint8_t value);
    void *io_ctx;
    
    /* Interrupt callback */
    void (*interrupt)(void *ctx, uint8_t vector);
    void *int_ctx;
    
    /* Statistics */
    uint64_t total_instructions;
    double total_time_ms;
    
    /* CUDA resources (if using GPU execution) */
    void *cuda_stream;
    bool use_gpu;
} gpucpu_ctx_t;

/*
 * ============================================================
 * API Functions
 * ============================================================
 */

/*
 * gpucpu_init - Initialize the GPU-CPU emulator
 * @ctx:        Context to initialize
 * @nm_ctx:     Near-memory context (with pseudoscopic device)
 * @ram_size:   Size of guest RAM in bytes
 *
 * Allocates x86 state and RAM in VRAM.
 */
int gpucpu_init(gpucpu_ctx_t *ctx, nearmem_ctx_t *nm_ctx, size_t ram_size);

/*
 * gpucpu_shutdown - Clean up emulator
 */
void gpucpu_shutdown(gpucpu_ctx_t *ctx);

/*
 * gpucpu_reset - Reset CPU to power-on state
 * @ctx:        Emulator context
 * @mode:       Initial mode (MODE_REAL for BIOS, MODE_LONG for direct 64-bit)
 */
void gpucpu_reset(gpucpu_ctx_t *ctx, int mode);

/*
 * gpucpu_load - Load binary into guest RAM
 * @ctx:        Emulator context
 * @data:       Binary data
 * @size:       Size in bytes
 * @address:    Load address in guest RAM
 */
int gpucpu_load(gpucpu_ctx_t *ctx, const void *data, size_t size, uint64_t address);

/*
 * gpucpu_set_entry - Set execution entry point
 */
void gpucpu_set_entry(gpucpu_ctx_t *ctx, uint64_t rip);

/*
 * gpucpu_run - Run emulation
 * @ctx:            Emulator context
 * @max_instructions: Stop after this many instructions (0 = unlimited)
 *
 * Runs until HLT, interrupt, or instruction limit.
 * Returns number of instructions executed.
 */
uint64_t gpucpu_run(gpucpu_ctx_t *ctx, uint64_t max_instructions);

/*
 * gpucpu_step - Execute single instruction
 */
int gpucpu_step(gpucpu_ctx_t *ctx);

/*
 * gpucpu_interrupt - Inject interrupt
 */
void gpucpu_interrupt(gpucpu_ctx_t *ctx, uint8_t vector);

/*
 * gpucpu_get_reg - Get register value
 */
uint64_t gpucpu_get_reg(gpucpu_ctx_t *ctx, int reg);

/*
 * gpucpu_set_reg - Set register value
 */
void gpucpu_set_reg(gpucpu_ctx_t *ctx, int reg, uint64_t value);

/*
 * gpucpu_read_mem - Read guest memory
 */
int gpucpu_read_mem(gpucpu_ctx_t *ctx, uint64_t addr, void *buf, size_t size);

/*
 * gpucpu_write_mem - Write guest memory
 */
int gpucpu_write_mem(gpucpu_ctx_t *ctx, uint64_t addr, const void *buf, size_t size);

/*
 * gpucpu_dump_state - Print CPU state for debugging
 */
void gpucpu_dump_state(gpucpu_ctx_t *ctx);

/*
 * ============================================================
 * Register Indices
 * ============================================================
 */

#define REG_RAX     0
#define REG_RCX     1
#define REG_RDX     2
#define REG_RBX     3
#define REG_RSP     4
#define REG_RBP     5
#define REG_RSI     6
#define REG_RDI     7
#define REG_R8      8
#define REG_R9      9
#define REG_R10     10
#define REG_R11     11
#define REG_R12     12
#define REG_R13     13
#define REG_R14     14
#define REG_R15     15
#define REG_RIP     16
#define REG_RFLAGS  17

/*
 * ============================================================
 * GPU Kernel Interface (for CUDA implementation)
 * ============================================================
 */

#ifdef __CUDACC__

/*
 * x86_decode_kernel - Decode instructions in parallel
 *
 * Each thread decodes one instruction from the instruction stream.
 * Results cached for repeated execution.
 */
__global__ void x86_decode_kernel(
    const uint8_t *code,
    x86_decoded_t *decoded,
    size_t n_instructions
);

/*
 * x86_execute_kernel - Execute micro-operations
 *
 * Each warp executes one x86 instruction (broken into micro-ops).
 * Memory operations coalesced for bandwidth efficiency.
 */
__global__ void x86_execute_kernel(
    x86_state_t *state,
    uint8_t *ram,
    const x86_decoded_t *decoded,
    int single_step
);

/*
 * x86_memcpy_kernel - Accelerated memory copy (REP MOVS)
 *
 * Tiled parallel memory copy leveraging GPU bandwidth.
 * 14Ã— faster than CPU for large copies.
 */
__global__ void x86_memcpy_kernel(
    uint8_t *ram,
    uint64_t dst,
    uint64_t src,
    uint64_t count,
    int direction
);

/*
 * x86_memset_kernel - Accelerated memory set (REP STOS)
 */
__global__ void x86_memset_kernel(
    uint8_t *ram,
    uint64_t dst,
    uint8_t value,
    uint64_t count
);

#endif /* __CUDACC__ */

#ifdef __cplusplus
}
#endif

#endif /* _GPUCPU_H_ */
