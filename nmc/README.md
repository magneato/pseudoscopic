```
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                                                   â•‘
    â•‘         â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—                                             â•‘
    â•‘         â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•                                             â•‘
    â•‘         â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘                                                  â•‘
    â•‘         â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘                                                  â•‘
    â•‘         â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—                                             â•‘
    â•‘         â•šâ•â•  â•šâ•â•â•â•â•šâ•â•     â•šâ•â• â•šâ•â•â•â•â•â•                                             â•‘
    â•‘                                                                                   â•‘
    â•‘                 Near-Memory Computing Primitives                                  â•‘
    â•‘         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â•‘
    â•‘         Where data stays put, and algorithms come calling                         â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

*A pseudoscopic subsystem for asymmetric memory patterns.*

---

## â—ˆ The Core Insight

Every GPU programmer knows the ritual dance:

```c
// The Traditional Waltz â€” A Dance in Three Movements
cudaMemcpy(gpu_data, host_data, size, cudaMemcpyHostToDevice);  // 50 GB â†’ (4 seconds)
kernel<<<blocks, threads>>>(gpu_data);                          // 0.2 seconds (Fast!)
cudaMemcpy(host_data, gpu_data, size, cudaMemcpyDeviceToHost);  // â† 50 GB (4 seconds)
```

For a *single* operation, you move **100 GB** across PCIe to process 50 GB of data. The math doesn't lie.

Want to run 10 operations on that dataset? That's **1 TB** of PCIe traffic. Your GPU spends 97% of its time as a glorified DMA controller, waiting for bytes that are already where they need to beâ€”they just live in the wrong address space.

> **Historical perspective**: This pattern emerged from the GPGPU era (~2007) when GPU memory was truly foreign territory. CUDA provided cudaMemcpy because there was no alternative. But hardware has evolved. Software assumptions haven't.

---

## â—ˆ The Inversion

**NMC inverts the data-to-compute relationship:**

```c
// The NMC Way â€” Data Stays Home
nmc_load(region, host_data, size);      // 50 GB â†’ (ONCE, amortized across operations)

for (int i = 0; i < 10; i++) {
    nmc_search(region, pattern[i], ...);  // 0 GB â€” data stays in VRAM
    nmc_extract(results, &count, 8);      // 8 bytes â† (just the answer)
}
```

Total PCIe traffic: **50 GB + 80 bytes** instead of 1 TB.

That's not a 2Ã— improvement. That's a **20,000Ã—** reduction in data movement.

---

## â—ˆ The Mathematics of Bandwidth

Let's be precise about why this matters:

```
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                        BANDWIDTH ASYMMETRY                               â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘                                                                         â•‘
    â•‘    GPU Internal Bandwidth (HBM2): ..............  900 GB/s             â•‘
    â•‘    GPU Internal Bandwidth (GDDR6X): ............  700 GB/s             â•‘
    â•‘    PCIe 4.0 x16 (practical): ...................   25 GB/s             â•‘
    â•‘    PCIe 3.0 x16 (practical): ...................   12 GB/s             â•‘
    â•‘                                                                         â•‘
    â•‘    Ratio (HBM2 / PCIe 3.0): .....................  75:1                â•‘
    â•‘                                                                         â•‘
    â•‘    Every byte that DOESN'T cross PCIe is a 75Ã— effective gain.         â•‘
    â•‘                                                                         â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

This asymmetry is the foundation of near-memory computing. The GPU's internal memory system operates at frequencies and parallelism that PCIe simply cannot match. When your workload is "read a lot, output a little," keeping data on the *fast* side of the divide is pure win.

---

## â—ˆ The Physical Architecture

NMC builds on Pseudoscopic's VRAM exposure:

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                              GPU Card                                          â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                                â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚   â”‚                         VRAM (HBM2 / GDDR6X)                          â”‚   â”‚
    â”‚   â”‚                                                                       â”‚   â”‚
    â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
    â”‚   â”‚   â”‚                      Data Region                            â”‚    â”‚   â”‚
    â”‚   â”‚   â”‚                                                             â”‚    â”‚   â”‚
    â”‚   â”‚   â”‚     Addressable via CPU:  mmap("/dev/psdisk0")             â”‚    â”‚   â”‚
    â”‚   â”‚   â”‚     Addressable via GPU:  CUDA device pointer               â”‚    â”‚   â”‚
    â”‚   â”‚   â”‚                                                             â”‚    â”‚   â”‚
    â”‚   â”‚   â”‚     â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚    â”‚   â”‚
    â”‚   â”‚   â”‚     â•‘           SAME PHYSICAL MEMORY                    â•‘  â”‚    â”‚   â”‚
    â”‚   â”‚   â”‚     â•‘           Different access paths                  â•‘  â”‚    â”‚   â”‚
    â”‚   â”‚   â”‚     â•‘           Same coherent view                      â•‘  â”‚    â”‚   â”‚
    â”‚   â”‚   â”‚     â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚    â”‚   â”‚
    â”‚   â”‚   â”‚                                                             â”‚    â”‚   â”‚
    â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
    â”‚   â”‚                              â”‚                 â”‚                      â”‚   â”‚
    â”‚   â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚
    â”‚   â”‚           â”‚                     â”‚    â”‚                     â”‚         â”‚   â”‚
    â”‚   â”‚      GPU Compute            CPU Access                               â”‚   â”‚
    â”‚   â”‚      (700 GB/s)             (12 GB/s via BAR1)                       â”‚   â”‚
    â”‚   â”‚      In-place ops           Control & results                        â”‚   â”‚
    â”‚   â”‚      Bulk transforms        Sparse reads                             â”‚   â”‚
    â”‚   â”‚                                                                       â”‚   â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                                                                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The CPU and GPU see the **same physical memory** through different paths:
- **CPU**: Write-combining access via PCIe BAR1 (mmap'd)
- **GPU**: Native device memory access (CUDA device pointer)

Synchronization is explicit memory barriersâ€”deterministic, measurable, no magic.

---

## â—ˆ API Usage

### Initialization

```c
nmc_context_t *ctx;
int result = nmc_init(NULL, -1, &ctx);  // Auto-detect device, auto-select GPU

// Or explicit:
result = nmc_init("/dev/psdisk0", 0, &ctx);  // Specific device, GPU 0
```

### Memory Allocation

```c
nmc_region_t *data;

// Allocate 50 GB in GPU VRAM
result = nmc_alloc(ctx, 50ULL << 30, NMC_MEM_GPU_ONLY, &data);

// Flags available:
// NMC_MEM_GPU_ONLY     â€” Data lives exclusively in VRAM
// NMC_MEM_CPU_VISIBLE  â€” Optimized for CPU access (but still in VRAM)
// NMC_MEM_STREAM       â€” Hint for streaming access pattern
```

### Data Loading (One-Time Transfer)

```c
// Load data into VRAM (this is the PCIe cost you pay once)
nmc_load(data, 0, my_huge_file, file_size);

// Ensure GPU can see it
nmc_sync(ctx, NMC_SYNC_CPU_TO_GPU);
```

### GPU Operations (No PCIe!)

After loading, all operations work in-place:

```c
// Search 50 GB for a pattern â€” data never moves
int64_t offsets[1024];
size_t match_count;
nmc_search(data, 0, file_size, "ERROR", 5, offsets, 1024, &match_count, NULL);

// Sort 10 billion floats in-place
nmc_sort_f32(data, 0, 10ULL << 30 / sizeof(float), NULL);

// Compute histogram â€” only 256 values returned
uint64_t histogram[256];
nmc_histogram_u8(data, 0, file_size, histogram, NULL);

// Reduce to sum â€” only 4 bytes returned
float sum;
nmc_reduce_sum_f32(data, 0, float_count, &sum, NULL);
```

### Extract Results (Sparse)

```c
// Only pull what you need
uint64_t match_offsets[100];
nmc_extract(data, first_match_offset, match_offsets, 100 * sizeof(uint64_t));
```

---

## â—ˆ Real-World Applications

### 1. Log Analysis at Scale

```
    Traditional Approach:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Load: 50 GB server logs
    Search: 100 different patterns
    PCIe traffic: 50 GB Ã— 100 operations Ã— 2 copies = 10 TB
    
    NMC Approach:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Load: 50 GB server logs (ONCE)
    Search: 100 different patterns (IN-PLACE)
    PCIe traffic: 50 GB + 100 KB results = 50.0001 GB
    
    Savings: 200Ã—
```

### 2. Database Cold Buffer

```c
// Overflow pages live in VRAM
nmc_region_t *cold_pages;
nmc_alloc(ctx, 32ULL << 30, NMC_MEM_GPU_ONLY, &cold_pages);

// GPU scans cold pages at 700 GB/s
// Only matching rows cross PCIe
nmc_search(cold_pages, 0, buffer_size, key, key_len, results, max, &count, NULL);
```

### 3. ML Feature Processing

```c
// 100 GB feature vectors loaded once
nmc_load(features, 0, feature_file, 100ULL << 30);

// GPU-accelerated normalization (in-place)
nmc_transform_normalize(features, 0, feature_count, mean, stddev, NULL);

// Histogram for distribution analysis (2 KB output)
nmc_histogram_f32(features, 0, feature_count, bins, 256, NULL);
```

### 4. LLM KV-Cache Spillover

```c
// When CPU RAM fills, KV-cache overflows to VRAM
// Pseudoscopic RAM mode handles migration automatically
// Attention computation accesses spilled cache in-place
```

### 5. Stream Processing

```c
// Continuous ingestion directly to VRAM
while (streaming) {
    nmc_load(buffer, write_offset, batch, batch_size);
    write_offset += batch_size;
    
    // GPU processes accumulated batches in-place
    nmc_histogram_u8(buffer, 0, write_offset, running_histogram, NULL);
}

// Only aggregates ever cross PCIe
```

---

## â—ˆ Performance Characteristics

| Operation | Data Size | PCIe Traffic | GPU Time | Notes |
|-----------|-----------|--------------|----------|-------|
| Search | 50 GB | 0 + results | ~100 ms | Pattern match at 500 GB/s |
| Sort | 10 GB | 0 | ~500 ms | Radix sort, in-place |
| Histogram | 50 GB | 2 KB | ~80 ms | 256-bucket byte histogram |
| Sum/Max | 10 GB | 4-8 bytes | ~20 ms | Single scalar output |
| Transform | 24 GB | 0 | ~50 ms | LUT-based byte transform |

Compare to traditional CUDA where each operation requires **2Ã— data size** in PCIe transfers.

---

## â—ˆ Building NMC

### Prerequisites

```bash
# 1. Load pseudoscopic driver in ramdisk mode
sudo modprobe pseudoscopic mode=ramdisk

# 2. Verify device exists
ls -la /dev/psdisk0   # Should exist

# 3. Verify CUDA is available
nvidia-smi            # Should show your GPU(s)
```

### Compilation

```bash
cd nmc
make

# With explicit CUDA path:
make CUDA_PATH=/opt/cuda-12.0
```

### Running Examples

```bash
# Log search demonstration
./build/log_grep /var/log/syslog "error" "warning" "failed"

# Analytics benchmark
./build/analytics

# Stream processing demo
./build/stream_demo
```

---

## â—ˆ Design Philosophy

> *"The fastest data transfer is the one that doesn't happen."*

NMC is built on the observation that modern workloads often:
1. **Load** large datasets (once)
2. **Perform** multiple operations (repeatedly)
3. **Extract** small results (aggregates, matches, statistics)

The traditional model optimizes for (1)â€”making uploads fast.
NMC optimizes for (2) and (3)â€”eliminating transfers entirely.

When your 50 GB dataset produces 50 KB of results after 10 operations, moving 50 GB ten times is **asymmetrically wasteful**.

**Asymmetric solutions for asymmetric problems.**

---

## â—ˆ Limitations (Honest Assessment)

1. **Initial load still crosses PCIe** â€” NMC optimizes repeated operations, not single-pass workflows
2. **CPU random reads are slow** â€” ~100ns per 64-byte line via BAR1; don't iterate byte-by-byte
3. **GPU memory is finite** â€” 16-80 GB depending on GPU model
4. **Requires pseudoscopic driver** â€” Not portable without the kernel module
5. **Not always faster** â€” Single-pass compute-bound workloads may not benefit

---

## â—ˆ Relationship to Pseudoscopic

NMC is a userspace library built on the [Pseudoscopic](../README.md) kernel driver. It requires pseudoscopic loaded in ramdisk mode:

```bash
sudo modprobe pseudoscopic mode=ramdisk
```

This exposes GPU VRAM as `/dev/psdisk0`, which NMC mmaps for CPU access while CUDA provides GPU access to the same physical memory.

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                          Pseudoscopic Stack                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   [ Your Application ]                                              â”‚
    â”‚          â”‚                                                          â”‚
    â”‚          â–¼                                                          â”‚
    â”‚   [ NMC Library ]    â† High-level primitives (search, sort, etc.)   â”‚
    â”‚          â”‚                                                          â”‚
    â”‚          â–¼                                                          â”‚
    â”‚   [ libnearmem ]     â† Core allocation and sync                     â”‚
    â”‚          â”‚                                                          â”‚
    â”‚          â–¼                                                          â”‚
    â”‚   [ pseudoscopic.ko ]  â† Kernel driver exposing VRAM                â”‚
    â”‚          â”‚                                                          â”‚
    â”‚          â–¼                                                          â”‚
    â”‚   [ /dev/psdisk0 ]   â† Block device backed by GPU VRAM              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

```
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                                               â•‘
    â•‘      "Data is a fluid. It seeks the path of least resistance.                 â•‘
    â•‘       The wise architect builds channels, not pumps."                         â•‘
    â•‘                                                                               â•‘
    â•‘                                        â€” Neural Splines Research, 2026        â•‘
    â•‘                                           Asymmetric Solutions                â•‘
    â•‘                                                                               â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## â—ˆ License

MIT License

Copyright Â© 2026 Neural Splines LLC

---

*"C is for cookie, and cookie is for me. But data is for VRAM, and VRAM is for GPU."* ğŸª
